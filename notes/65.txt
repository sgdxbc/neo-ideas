id
65
create
2024-06-25T20:33:48.553313452+08:00
update
2024-06-25T21:00:49.471060975+08:00
parent
58

基于以上的分析我想给出的结论就是，对于生成式任务，其计算量并不固定，因此模型应该具备进行不固定计算量的推理的能力。尤其是为了完成一次计算量远远大于信息量的任务，可以通过将网络反复不停地计算，将每次计算的结果作为下次计算的输入，并由网络自主决定计算次数的方式来完成。对于缺少循环语句的神经网络，这相当于把整个神经网络跑在一个大循环之内，从而任何局部的循环都可以通过大循环加上条件分支（这个神经网络可以有）的方式来模拟。

我认为这不应该是一个只局限于transformer的设计，也不是只能解决其关注的场景下的问题。这应当是一种通用的设计思路：这个世界上应该存在大量的神经网络都是像这样周而复始地去跑的。何况transformer还有它的计算量与答案长度绑定的局限性，我们应该对它改进，让它产生一个符号的计算量也可以不固定（比如说允许它输出一个符号时指定这个符号只是给它自己接下来做参考，不要包含在最终输出当中）。

如果想要设计一个通用的完成生成式任务的循环推理方案，一个首要的难题就是找到循环不变式，或者用数学的术语，归纳命题。不管是人手写循环还是transformer，其循环都是基于循环不变式设计的。循环不变式保证了每循环一次，我们就会离最终的目标结果更进一步。

一个普通的任意规模四则运算任务的循环不变式是很好找的：找出当前表达式中优先级最高的一个子表达式运算，将其替换为运算结果，其他部分照抄。这一定能让我们朝着最终目标更进一步，而且甚至还有着良好定义的终止条件：表达式不再包含运算就可以结束了。

类似地可以加入微分运算，但是一旦加入积分运算，这事情就变了：对一个式子施以分部积分法并不一定能带我们朝着最终目标更进一步。

当我们试图推广到任意的生成式任务时也会面临一样的问题：像是「降噪」或者「输出一个符号」这种的循环不变式，我们如何找到一个普适的版本对所有任务都适用？当然我们可以就此打住，只说「反正对于每个任务都要找到它对应的循环不变式，从而才能解锁无限计算量而像个真正的人类一样用不定计算量来完成这个任务」。但这就变成一篇纯口嗨的工作了。
