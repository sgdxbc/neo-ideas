id
63
create
2024-06-25T19:48:55.207275904+08:00
update
2024-06-25T20:57:21.356841925+08:00
parent
58

这样做显然是低效且不可延展（scalable）的。我们可以看到另一种思路：把一个神经网络重复不断地推理。

在stable diffusion中我们反复使用同一个神经网络，逐渐将随机噪声「降噪」为一张图片。这个过程中的每一步的输入都是由上一步的输出得来的（具体地说，上一步输出与上一步输入的线性叠加）。通过这种方法，在信息量不增加的前提下将计算量提升到所需的水平。

完成一幅风格图像的计算量并不固定。但是起码我们现在有机会把计算量定为某个上界，从而解决这一计算量远大于信息量的任务（就算交给人来画这也的确是个冗长枯燥的任务）。不过固定工作量的局限性还是显现出了端倪。

AI作画的复杂程度非常恒定。常常只是把画面元素的排布方式换一下的各种排列组合。这意味着如果一幅画对应的计算量/工作量远大/小于设定的计算量，那么神经网络画再多次也画不出这幅画。

对于一些特定画面元素（臭名昭著的：手），其所应当消耗的计算量远高于单位面积的其他元素（观察人类绘画行为可知）。神经网络（目前还似乎）做不到把计算量非均匀地分布到画面的各个地方。说到底，stable diffusion只是部分地模拟了人类先打线稿再上色再画光影这种一遍一遍的流程。至于在每一遍当中单独绘制画面各个元素的行为则没有任何对应——它只是对每个像素都一视同仁地降了个噪而已。

（懂了，下一代AI作画的研究方向是引入注意力（）
